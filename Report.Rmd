---
title: "ProjectReport PracticalML"
author: "Nilanka Weeraman"
date: "November 8, 2018"
output: rmarkdown::github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Import and examine dataset
```{r import files}
training <- read.csv("D:\\D\\F_BACKUP\\Coursera\\Axiata\\Practical ML\\pml-training.csv")
testing <- read.csv("D:\\D\\F_BACKUP\\Coursera\\Axiata\\Practical ML\\pml-testing.csv")

names(training)

```
#### Observations of training and testing data
The dataset contains sensor readings of activity (dumpbell lifting) for each person. And in training data, both continuous readings (sampled) and summarized readings (after activity is finished for one rep) available. This is distinguished by **new_window** variable. 
When examining testing data, it only contains new_window='No' instances only. Thus we can ignore 'No' instances for training.

```{r filtering no} 
tr_no<-training[training$new_window=='no',]

```
And I'm also keeping out timestamp related attributes from training, and define my training varible list

```{r define}
inputs=c('user_name','roll_belt','pitch_belt','yaw_belt','total_accel_belt'
         ,'gyros_belt_x','gyros_belt_y','gyros_belt_z','accel_belt_x','accel_belt_y'
         ,'accel_belt_z','magnet_belt_x','magnet_belt_y','magnet_belt_z','roll_arm'
         ,'pitch_arm','yaw_arm','total_accel_arm','gyros_arm_x','gyros_arm_y'
         ,'gyros_arm_z','accel_arm_x','accel_arm_y','accel_arm_z','magnet_arm_x'
         ,'magnet_arm_y','magnet_arm_z','roll_dumbbell','pitch_dumbbell','yaw_dumbbell'
         ,'total_accel_dumbbell','gyros_dumbbell_x','gyros_dumbbell_y','gyros_dumbbell_z','accel_dumbbell_x'
         ,'accel_dumbbell_y','accel_dumbbell_z','magnet_dumbbell_x','magnet_dumbbell_y','magnet_dumbbell_z'
         ,'roll_forearm','pitch_forearm','yaw_forearm','total_accel_forearm'
         ,'gyros_forearm_x','gyros_forearm_y','gyros_forearm_z','accel_forearm_x','accel_forearm_y'
         ,'accel_forearm_z','magnet_forearm_x','magnet_forearm_y','magnet_forearm_z','classe')
tr_no_fil <- tr_no[,inputs]

```
#### Explore covariates

Exploring relationship between inputs and 'target'classe' by plotting distributions as;
```{r,echo=FALSE, results='asis', message = FALSE, error = FALSE, warning= FALSE}
require(ggplot2)
#library(ggplot2)
#different measurements have different distributions per class
g<-ggplot(tr_no_fil,aes(classe,roll_belt))
g + geom_violin(scale="area")
```

```{r plot}
plot(tr_no_fil$classe,tr_no_fil$roll_belt)
```
It is clear that Class A and E have differences in distributions for variable roll_belt. 
Likewise its possible to explore few more

Even different users have different measurements for same activity type, as shown in below plot. 
Thus 'user_name' can be considered as an input   
```{r, plotly=TRUE}
plot(tr_no_fil[tr_no_fil$classe=='A',c("user_name")],tr_no_fil[tr_no_fil$classe=='A',c("yaw_dumbbell")])
```
Since we have large no of potential input variables, we can us   

Lets examine a simple classifier is able to classify events.   
Using a decision tree    
     
libray(caret)    
modTree<-train(classe ~., method="rpart",data=training)    
require(rattle)    
fancyRpartPlot(modTree$finalModel)    
![simple decision tree showing class A and E can be seperated with high accuracy](D:\D\F_BACKUP\Coursera\Axiata\Practical ML\DTPlot.png)    


Thus I've decided to use Random Forest classifier for predictions      
*Note - I could not include caret package into R markdown. Thus below codes are shown as R comments. Please bear with me *    

#### Model Building and Evaluation

library(caret)   
inTrain = createDataPartition(tr_no$classe, p = 3/4)[[1]]   
training = tr_no[ inTrain,]   
testing = tr_no[-inTrain,]   
training<-training[,inputs]   
testing<-testing[,inputs]   

metric<-"Accuracy"   
## 10 fold cross validation to negate any overfitting by random Forest model   
control <- trainControl(method="repeatedcv", number=10, repeats=3, search="random")   
mtry <- sqrt(ncol(training))   
modRF <- train(classe~., data=training, method="rf",    
               metric=metric, tuneLength=15, trControl=control)   
                   
predRF<-predict(modRF,testing)
confusionMatrix(testing$classe,predRF)


refence - https://machinelearningmastery.com/tune-machine-learning-algorithms-in-r/

Now we test performance on testing dataset    
     
     
![confusionMatrix](D:\D\F_BACKUP\Coursera\Axiata\Practical ML\ConfusionM.PNG)


Both in sample and out of sample error rates are very low. Thus I have finalized above model as my final and used for the quiz.     


